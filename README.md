# ğŸ¤– FastAPI LLM Microservice

A production-ready FastAPI microservice featuring **conversational AI**, **RAG (Retrieval-Augmented Generation)**, and **asynchronous background job processing**. Built with modern Python best practices and enterprise-level architecture.

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.123.5-green.svg)](https://fastapi.tiangolo.com)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://docker.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸŒŸ Features

### Core Capabilities
- **ğŸ—¨ï¸ Conversational AI**: Multi-turn chat with streaming responses using Google Gemini
- **ğŸ“š RAG Implementation**: Document upload, chunking, vectorization, and intelligent retrieval
- **ğŸ” JWT Authentication**: Secure endpoints with access/refresh token mechanism
- **âš¡ Async Processing**: Background tasks with Celery for document indexing and user profiling
- **ğŸ“Š Vector Search**: PostgreSQL with pgvector extension for similarity search
- **ğŸ”„ Real-time Updates**: Server-Sent Events (SSE) for streaming chat responses

### Technical Highlights
- **Microservices Architecture**: Containerized services with Docker Compose
- **Persistent Session Management**: Redis-based conversation history
- **User Profiling**: Automated background analysis of user conversations
- **Document Processing**: PDF parsing, text chunking, and embedding generation
- **Production-Ready**: Gunicorn with Uvicorn workers, proper error handling

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI   â”‚â”€â”€â”€â”€â–¶â”‚    Redis     â”‚     â”‚  PostgreSQL â”‚
â”‚   Service   â”‚     â”‚   (Broker)   â”‚     â”‚  + pgvector â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚                     â”‚
       â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Celery Worker  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tech Stack

| Component | Technology |
|-----------|-----------|
| **Web Framework** | FastAPI 0.123.5 |
| **LLM Provider** | Google Gemini (gemini-2.5-flash) |
| **Embeddings** | Google Generative AI Embeddings |
| **Vector Database** | PostgreSQL 16 + pgvector |
| **Message Broker** | Redis 7.2 |
| **Task Queue** | Celery 5.5.3 |
| **Authentication** | JWT (python-jose) |
| **Document Processing** | LangChain, PyPDF |
| **Containerization** | Docker + Docker Compose |

## ğŸ“‹ Prerequisites

- Docker & Docker Compose
- Google AI API Key ([Get one here](https://ai.google.dev/))
- Python 3.11+ (for local development)

## ğŸš€ Quick Start

### 1. Clone the Repository
```bash
git clone https://github.com/ilkaygrgn/FastAPI-LLM-Microservice.git
cd FastAPI-LLM-Microservice
```

### 2. Set Environment Variables
Create a `.env` file:
```bash
GOOGLE_API_KEY=your_google_api_key_here
SECRET_KEY=your_secret_key_for_jwt
OPENAI_API_KEY=your_openai_key_optional
```

### 3. Start Services
```bash
docker-compose up -d
```

Services will be available at:
- **API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs
- **PostgreSQL**: localhost:5432
- **Redis**: localhost:6379

## ğŸ”‘ API Endpoints

### Authentication
| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/auth/register` | Register new user |
| POST | `/api/v1/auth/login` | Login and get tokens |
| POST | `/api/v1/auth/refresh` | Refresh access token |

### Chat
| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/llm/chat` | Stream chat response with RAG |

### Document Management
| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/llm/upload-document` | Upload PDF for RAG indexing |

### User Management
| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/v1/users/me` | Get current user profile |

### Background Jobs
| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/jobs/start` | Start a background task |
| GET | `/api/v1/jobs/{job_id}` | Check job status |

## ğŸ’¡ Usage Examples

### 1. Register & Login
```bash
# Register
curl -X POST http://localhost:8000/api/v1/auth/register \
  -H "Content-Type: application/json" \
  -d '{
    "email": "user@example.com",
    "password": "secure_password",
    "full_name": "John Doe"
  }'

# Login
curl -X POST http://localhost:8000/api/v1/auth/login \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=user@example.com&password=secure_password"
```

### 2. Upload Document for RAG
```bash
curl -X POST http://localhost:8000/api/v1/llm/upload-document \
  -H "Authorization: Bearer YOUR_ACCESS_TOKEN" \
  -F "file=@document.pdf"
```

### 3. Chat with RAG
```bash
curl -X POST http://localhost:8000/api/v1/llm/chat \
  -H "Authorization: Bearer YOUR_ACCESS_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What does the document say about...?",
    "session_id": "session_123"
  }'
```

## ğŸ—‚ï¸ Project Structure

```
fastapi-llm-microservice/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚       â”œâ”€â”€ auth.py          # Authentication endpoints
â”‚   â”‚       â”œâ”€â”€ users.py         # User management
â”‚   â”‚       â”œâ”€â”€ llm.py           # Chat & document upload
â”‚   â”‚       â””â”€â”€ jobs.py          # Background job management
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”‚   â””â”€â”€ security.py          # JWT & password hashing
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ database.py          # Database connection
â”‚   â”‚   â””â”€â”€ models.py            # SQLAlchemy models
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ user.py              # User schemas
â”‚   â”‚   â””â”€â”€ chat.py              # Chat request/response
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ llm_service.py       # LLM integration & streaming
â”‚   â”‚   â”œâ”€â”€ vector_db_service.py # RAG & embeddings
â”‚   â”‚   â””â”€â”€ chat_history.py      # Conversation persistence
â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â”œâ”€â”€ worker.py            # Celery app configuration
â”‚   â”‚   â””â”€â”€ tasks.py             # Background tasks
â”‚   â””â”€â”€ main.py                  # FastAPI application
â”œâ”€â”€ docker-compose.yml           # Multi-container orchestration
â”œâ”€â”€ Dockerfile                   # Application container
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `GOOGLE_API_KEY` | Google AI API key | Required |
| `SECRET_KEY` | JWT secret key | Required |
| `DATABASE_URL` | PostgreSQL connection string | Auto-configured in Docker |
| `REDIS_HOST` | Redis server hostname | `redis` |
| `GOOGLE_LLM_MODEL` | Gemini model name | `gemini-2.5-flash` |

### Docker Compose Services

- **api**: FastAPI application (4 Gunicorn workers)
- **worker**: Celery worker for background tasks
- **db**: PostgreSQL 16 with pgvector extension
- **redis**: Redis 7.2 for session storage & task queue

## ğŸ“Š Database Schema

### Users Table
```sql
- id: SERIAL PRIMARY KEY
- email: VARCHAR UNIQUE
- hashed_password: VARCHAR
- full_name: VARCHAR
- user_profile: TEXT (AI-generated from conversations)
- is_active: BOOLEAN
- created_at: TIMESTAMP
```

### Vector Store (pgvector)
- `langchain_pg_collection`: Document collections
- `langchain_pg_embedding`: Vector embeddings with metadata

## ğŸ§ª Testing

### Health Check
```bash
curl http://localhost:8000/
# Response: {"status":"ok","service":"FastAPI LLM Microservice"}
```

### Check Running Containers
```bash
docker-compose ps
```

### View Logs
```bash
# All services
docker-compose logs -f

# Specific service
docker-compose logs -f api
docker-compose logs -f worker
```

## ğŸ” Security Features

- **Password Hashing**: Bcrypt with salt
- **JWT Tokens**: Short-lived access tokens (30 min) + refresh tokens (7 days)
- **Protected Endpoints**: Bearer token authentication
- **SQL Injection Prevention**: SQLAlchemy ORM
- **CORS Configuration**: Configurable allowed origins

## ğŸš§ Known Limitations

- **Google API Quota**: Free tier has embedding limits (1,500 requests/day)
- **File Upload**: Currently supports PDF only
- **Streaming**: SSE may not work with some reverse proxies

## ğŸ›£ï¸ Roadmap

- [ ] Multi-LLM support (Claude, OpenAI, local models)
- [ ] OpenAI embeddings fallback
- [ ] Document chunking strategies (semantic, sliding window)
- [ ] Conversation summarization
- [ ] Rate limiting
- [ ] Prometheus metrics
- [ ] Kubernetes deployment configs

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¤ Author

**Ilkay Girgin**
- GitHub: [@ilkaygrgn](https://github.com/ilkaygrgn)
- LinkedIn: [linkedin.com/in/ilkaygirgin](https://linkedin.com/in/ilkaygirgin)

## ğŸ™ Acknowledgments

- [FastAPI](https://fastapi.tiangolo.com/) - Modern web framework
- [LangChain](https://langchain.com/) - LLM application framework
- [Google AI](https://ai.google.dev/) - Gemini API
- [pgvector](https://github.com/pgvector/pgvector) - Vector similarity search

---

â­ **If you find this project useful, please consider giving it a star!**